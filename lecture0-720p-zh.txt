<img src="https://prod-edx-ai-translations-assets.s3.amazonaws.com/google-translate.png" alt="由 Google 徽标翻译"><br><br>[自动生成]<br>[音乐]
于布莱恩：好的。
欢迎大家阅读《Python 人工智能简介》。
我叫于布莱恩。
在本课程中，我们将探讨一些想法和技术，
以及作为人工智能基础的算法。
现在，人工智能涵盖了多种类型的技术。
每当您看到计算机执行以下操作时
在某种程度上显得聪明或理性，
就像认出照片中某人的脸一样
或者能够比人们更好地玩游戏，
或者当我们用手机说话时能够理解人类语言
他们理解我们的意思并能够回应我们，
这些都是人工智能的例子。
在本课程中，我们将探讨一些使人工智能成为可能的想法。
所以我们将从搜索开始我们的对话。
问题是，我们有人工智能，我们会
就像人工智能能够寻找某种问题的解决方案，
不管这个问题是什么。
无论是尝试获取从 A 点到 B 点的行车路线，
或者试图弄清楚如何玩游戏，
例如，玩井字棋游戏，弄清楚要走什么步
它应该使。
接下来我们就来看看知识。
理想情况下，我们希望我们的人工智能能够了解信息，
能够表示该信息，
更重要的是，能够从这些信息中得出推论。
能够使用它所知道的信息并得出额外的结论。
因此，我们将讨论如何对人工智能进行编程来做到这一点。
然后我们将探讨不确定性的话题。
谈论如果计算机不确定事实会发生什么的想法
但也许只能以一定的概率确定？
所以我们将讨论概率背后的一些想法
以及计算机如何开始处理不确定事件
从这个意义上来说，也是为了变得更聪明一点。
之后，我们将把注意力转向优化。
当计算机尝试针对某种目标进行优化时出现的问题，
尤其是在可能存在的情况下
计算机可以通过多种方式解决问题，
但我们正在寻找更好的方法，或者可能是最好的方法
如果可能的话。
然后我们将了解机器学习，或者更普遍的学习。
看看我们何时可以访问数据
通过从数据中学习，我们的计算机可以被编程为非常智能
并从经验中学习，能够更好地完成任务
基于对数据的更多访问。
例如，您的电子邮件收件箱在哪里
知道您的哪些电子邮件是好电子邮件，哪些电子邮件是垃圾邮件。
这些都是计算机的例子
能够从过去的经验和过去的数据中学习。
我们还将了解计算机如何汲取灵感
从人类的智力，看人脑的结构
以及神经网络如何成为类似这种想法的计算机。
以及如何利用计算机的某种结构
程序，我们可以编写神经网络
能够非常非常有效地执行任务。
最后，我们将把注意力转向语言。
不是编程语言，而是我们每天所说的人类语言。
看看即将到来的挑战
就像计算机试图理解自然语言一样
以及它是如何成为自然语言的
现代人工智能中发生的处理
实际上可以工作。
但今天我们将从搜索开始对话。
这个问题试图弄清楚什么
当我们遇到计算机所处的某种情况时要做的事情，
可以这么说，代理所处的某种环境。
我们希望该代理能够以某种方式寻找解决方案
到那个问题。
现在，这些问题可能以多种不同类型的形式出现。
例如，一个例子可能类似于经典的 15
与您可能会遇到的滑动瓷砖难题
已经看到，您试图滑动瓷砖以确保
所有数字按顺序排列。
这是您可能称之为搜索问题的示例。
15 个谜题开始于最初的混合状态
我们需要某种方法来找到要采取的行动才能返回
将谜题恢复到已解决状态。
但也存在类似的问题，您可以通过其他方式来解决。
例如，试图找到穿过迷宫的路，
是搜索问题的另一个例子。
你从一个地方开始，你有一些你想要达到的目标，
你需要找出正确的行动顺序
从初始状态到目标。
虽然这有点抽象，但任何时候
我们在这堂课上讨论迷宫解决，
你可以把它翻译成更真实的世界，
诸如行车路线之类的东西。
如果您想知道 Google 地图如何能够找出什么
是您从 A 点到达 B 点的最佳方式
例如，根据交通状况，什么时间、什么时间播放。
它通常是某种搜索算法。
你有一个人工智能试图从初始位置开始
通过采取一系列行动来实现某种目标。
所以我们今天的谈话将通过思考开始
关于这些类型的搜索问题以及什么
为了人工智能，需要解决这样的搜索问题
以便能够找到一个好的解决方案。
不过，为了做到这一点，我们需要
介绍一些术语，其中一些
我已经用过了。
但我们首先需要考虑的是代理。
代理只是感知其环境的一些实体，
它能够以某种方式感知周围的事物，
并以某种方式对该环境采取行动。
因此，就行车路线而言，
您的代理可能是某辆车的代表
试图弄清楚要采取什么行动才能
到达目的地。
对于带有滑动瓷砖的 15 个拼图，
代理可能是人工智能或试图解决这个难题的人，
试图找出要移动哪些方块才能找到该解决方案。
接下来，我们介绍一下状态的概念。
状态只是代理在其环境中的一些配置。
例如，在 15 个谜题中，任何状态都可能是这三个中的任何一个
例如。
状态只是图块的一些配置。
这些状态中的每一个都是不同的并且正在发生
需要稍微不同的解决方案。
每一项都需要采取不同的行动顺序
从这个初始状态到达目标，
这就是我们想要达到的目标。
那么初始状态。
那是什么？
初始状态就是智能体开始的状态。
我们将从这样一个状态开始
这将是我们搜索算法的起点，
可以这么说。
我们将从这个初始状态开始
然后开始推理，思考我们可以采取什么行动
到初始状态，以便弄清楚如何从头开始
到最后，从最初的位置到我们的目标是什么。
我们如何从最初的位置走向目标？
最终，这是通过采取行动来实现的。
行动只是我们在任何特定状态下可以做出的选择。
在人工智能中，我们总是会尝试将这些想法形式化一点
更准确地说，我们可以对它们进行更多编程
从数学上来说，可以这么说。
所以这将是一个反复出现的主题，我们可以更精确地定义行动
作为一个函数。
我们将有效地定义一个名为 actions 的函数
接受输入 S，其中 S 将是存在于内部的某种状态
我们的环境，S 的动作将以状态作为输入
并返回作为输出的所有操作的集合
可以在该状态下执行。
因此某些操作可能仅在某些状态下有效
而其他州则不然。
我们很快也会看到这样的例子。
以 15 号拼图为例，
它们通常有四种可能的行动
我们大多数时候都可以做到。
我们可以向右滑动一个图块，向左滑动一个图块，
例如，向上滑动图块或向下滑动图块。
这些将是我们可以采取的行动。
所以不知怎的，我们的人工智能，我们的程序，需要一些编码
国家的信息，通常采用某种数字格式，
以及这些动作的一些编码。
但它还需要对这些事物之间的关系进行一些编码，
状态和动作如何相互关联？
为了做到这一点，我们将介绍我们的人工智能
一个转换模型，它将描述什么状态
我们在其他状态执行一些可用操作后得到的结果。
再说一次，我们可以更精确一点，
再次更正式地定义这个转换模型，
作为一个函数。
该函数将是一个名为 result 的函数，
这次需要两个输入。
第一个输入是 S，某种状态。
输入第二个是 A，一些动作。
这个函数结果的输出是
它会给我们在状态下执行操作 A 后得到的状态
S.所以让我们看一个例子来更准确地了解这实际上是什么
方法。
例如，下面是 15 拼图的状态示例。
这是一个操作示例，将图块向右滑动。
如果我们将这些作为输入传递给结果函数会发生什么？
同样，结果函数将这个板、这个状态作为它的第一个输入。
它需要一个动作作为第二个输入。
当然，在这里，我用视觉方式描述事物，所以
您可以直观地看到状态是什么以及操作是什么。
在计算机中，您可能代表以下操作之一
只是代表动作的一些数字。
或者如果您熟悉允许的枚举
你去列举多种可能性，可能就是这样的。
状态可能只是表示为数组或二维数组，
所有这些存在的数字。
但在这里我们将以视觉方式展示它，以便您可以看到它。
当我们采取这个状态和这个动作时，将其传递到结果函数中，
输出是一个新的状态。
我们拿一块图块并将其向右滑动后得到的状态，这是
是我们得到的结果。
例如，如果我们有不同的动作和不同的状态，
并将其传递到结果函数中，
我们会得到完全不同的答案。
所以需要注意结果函数
弄清楚如何采取某种状态并采取行动并获得什么结果。
这将是我们的过渡模型
描述状态和动作如何相互关联。
如果我们采用这个转变模型并更广泛地思考它
在整个问题中，我们可以形成我们所谓的状态空间，
我们可以从初始状态得到的所有状态的集合
通过任何一系列的行动，通过采取零个或一个或两个或更多个
除此之外的动作，所以我们可以画一个图表
看起来像这样。
每个州都由游戏板代表。
并且有箭头将每个状态连接到我们所知道的每个其他状态
可以从该状态得到两个。
而且状态空间比您在这里看到的要大得多。
这只是状态空间实际情况的一个示例。
一般来说，在许多搜索问题中，
无论是这个特定的 15 个谜题还是行车路线
或者其他什么，状态空间看起来像这样。
我们有单独的状态和连接它们的箭头。
通常，为了简单起见，我们会
简化我们对整个事情的表示
作为图形，节点和连接节点的边的某种序列。
但您可以将这种更抽象的表示视为完全相同的想法。
每个小圆圈或节点都是
将代表我们问题中的一个州。
这里的箭头代表动作
我们可以在任何特定的状态下采取，
例如，将我们从一个特定状态带到另一个状态。
好的。
现在我们有了代表这些状态的节点的想法，
可以将我们从一种状态带到另一种状态的行动，
以及定义发生情况的转换模型
在我们采取特定行动之后。
所以我们下一步需要弄清楚的是
我们如何知道人工智能何时解决了问题。
我需要某种方式来知道人工智能何时达到目标，
它找到了目标。
接下来我们需要将其编码到人工智能中
是目标测试，确定给定状态是否是目标状态的某种方法。
对于行车路线之类的情况，这可能非常简单。
如果你处于与任何事物相对应的状态
用户输入了他们的预期目的地，嗯，
然后你就知道你处于目标状态。
在 15 谜题中，它可能正在检查数字
确保它们都按升序排列。
但人工智能需要某种方式来编码是否
他们碰巧处于的任何状态都是一个目标。
有些问题可能只有一个目标，比如迷宫
你有一个初始位置和一个结束位置
这就是目标。
在其他更复杂的问题中，您可能
想象有多种可能的目标，有多种方法
解决问题。
我们可能不关心计算机找到哪一个
只要它确实找到了特定的目标。
然而，有时计算机不仅仅关心寻找目标，
但要找到一个好的目标，或者一个低成本的目标。
正是因为这个原因，最后一块
我们用来定义这些搜索问题的术语
是一种称为路径成本的东西。
您可能会想象，就行车路线而言，
如果我说我想要从 A 点出发的路线，那会很烦人
到B点，谷歌地图给我的路线是一条很长的路线，有很多
走了不必要的弯路，花费了比应有的时间更长的时间
我必须到达那个目的地。
正是由于这个原因，当我们制定搜索问题时，
我们经常会给每条路径某种数字成本，一些数字告诉我们
采取这种特殊选择的成本有多大。
然后告诉我们的人工智能，不仅仅是寻找解决方案，
从初始状态到达目标的某种方式，
我们真的很想找到一种能够最小化这条路径成本的方法
更便宜，或者花费更少的时间，或者最大限度地减少
一些其他数值。
如果我们再次看一下这张图，我们可以用图形来表示这一点。
想象一下每一个箭头，每一个动作
我们可以从一个州带到另一个州，
有某种与之相关的数字，
该数字是该特定操作的路径成本，其中
任何特定行动的一些成本
例如，可能比其他一些行动的成本更昂贵。
尽管这只会在某些问题中发生。
在其他问题中我们可以简化图表
并且假设任何特定行动的成本都是相同的。
这可能就是 15 个谜题的情况，
例如，我是否真的没有什么区别
向右移动或向左移动。
唯一重要的是总步数
从 A 点到 B 点我必须采取这些步骤。
是同等成本的。
我们可以假设这是一项恒定成本，例如。
所以这现在构成了我们的基础
可能会被认为是一个搜索问题。
搜索问题有某种初始状态，某个地方
我们开始，我们可以采取某种行动
或我们在任何给定状态下可以采取的多项行动，
它有一个转换模型，某种定义方式
当我们从一种状态开始采取一项行动时会发生什么，
结果我们会处于什么状态。
除此之外，我们还需要一些目标测试来了解是否
我们已经达到了目标。
然后我们需要一个路径成本函数来告诉我们任何特定的路径，
通过遵循一些行动序列，该路径的成本有多高。
就金钱或时间而言，它的成本是多少？
或我们试图尽量减少使用的其他一些资源。
最终的目标是找到解决方案，本例中的解决方案
只是一些将我们带离初始状态的动作序列
到目标状态。
而且，理想情况下，我们不仅希望找到任何解决方案，而且希望找到最佳解决方案，
这是所有解决方案中路径成本最低的解决方案
可能的解决方案。
并且在某些情况下，可能存在多个最优解，
但最优解仅仅意味着存在
在寻找解决方案方面我们不可能做得更好。
现在我们已经定义了问题。
现在我们需要开始弄清楚如何
我们要解决的是这种搜索问题。
为了做到这一点，你可能会想象
我们的计算机需要表示一大堆数据
关于这个特定问题。
我们需要表示有关问题所在位置的数据。
我们可能需要同时考虑多种不同的选择。
通常当我们试图打包一大堆相关数据时
一起达到一个状态，我们将使用数据结构来做到这一点
我们将称之为节点。
节点是一种数据结构
跟踪各种不同的值，
特别是在搜索问题的情况下，
它将特别跟踪这四个值。
每个节点都会跟踪一个状态，即我们当前所处的状态。
每个节点也会跟踪父节点。
父级是我们之前的状态或节点
我们为了达到当前状态而使用的。
这将是相关的，因为最终，
一旦我们到达目标节点，一旦我们到达终点，
我们想知道为了实现该目标我们采取了哪些行动顺序。
我们要知道这一点的方法就是看看这些父母
跟踪是什么引导我们实现目标，以及是什么引导我们达到那种状态，
是什么导致我们进入之前的状态，等等，
回溯到起点，以便我们
了解我们需要采取的整个行动顺序
从头到尾。
该节点还将跟踪我们为了获得结果而采取的操作
从父状态到当前状态。
该节点还将跟踪路径成本。
换句话说，它将跟踪记录的数字
表示从初始状态到状态需要多长时间
我们目前恰好处于这个位置。
当我们开始讨论一些问题时，我们就会明白为什么这是相关的
我们可以针对这些搜索问题进行哪些优化
一般来说。
这就是我们要采用的数据结构
为了解决问题而使用。
现在让我们谈谈方法，我们如何
真正开始解决问题了吗？
好吧，正如你可能想象的那样，我们要做什么
我们要从一个特定的状态开始
我们将从那里开始探索。
直觉是，从给定的状态来看，
我们有多种选择，
我们将探索这些选择。
一旦我们探索这些选项，我们就会发现更多的选项
将让自己变得可用。
我们将考虑所有可用的选项
存储在我们称为前沿的单个数据结构内。
边界将代表所有事物
我们接下来可以探索的，我们还没有探索或访问过的。
所以在我们的方法中，我们将开始这个搜索
算法从仅包含一个状态的边界开始。
边界将包含初始状态，因为在一开始，
这是我们所知道的唯一状态。
这是唯一存在的状态。
然后我们的搜索算法将有效地遵循一个循环。
我们将一次又一次地重复某些过程。
我们要做的第一件事是如果边界是空的
那么就没有解决办法了。
我们可以报告说，没有办法达到目标。
这当然是可能的。
人工智能可能会遇到某些类型的问题
尝试探索并意识到没有办法解决这个问题。
这对于人类来说也是有用的信息。
所以如果边界是空的，那就意味着没有什么可以探索的，
而且我们还没有找到解决方案，所以没有解决方案。
没有什么可以探索的了。
否则我们要做的就是从边界中删除一个节点。
所以现在一开始，边界
仅包含一个代表初始状态的节点。
但随着时间的推移，边界可能会扩大。
它可能包含多个状态。
所以在这里我们只是从该边界删除一个节点。
如果该节点恰好是目标，那么我们就找到了解决方案。
因此，我们从边界中删除一个节点并问自己，这是目标吗？
我们通过应用我们之前讨论过的目标测试来做到这一点，
询问我们是否已到达目的地或询问 15 的所有号码是否已到达
谜题恰好是有序的。
因此，如果该节点包含目标，我们就找到了解决方案。
伟大的。
我们完成了。
否则，我们需要做的是扩展节点。
这是人工智能中的一个术语。
扩展节点仅意味着查看该节点的所有邻居。
换句话说，考虑所有可能的行动
我可以从该节点代表的状态中获取
以及我可以从那里到达哪些节点。
我们将获取所有这些节点，即下一个节点
我可以从当前正在查看的内容中找到它，
并将它们添加到边界。
然后我们将重复这个过程。
所以在很高的层面上，我们的想法是我们从一个前沿开始
包含初始状态。
我们不断地从边界中删除一个节点，
看看我们下一步可以到达哪里，并将这些节点添加到边界，
一遍又一遍地重复这个过程，直到我们删除
来自边界的一个节点，它包含一个目标，这意味着我们已经解决了
问题。
或者我们遇到了边境空无一人的情况，此时
我们没有解决办法。
那么让我们实际尝试一下伪代码，
通过查看示例搜索问题的示例将其付诸实践。
所以我这里有一个示例图。
A 通过此操作连接到 B，B 连接到节点 C 和 D、C
连接到 D，E 连接到 F。我想做的
让我的 AI 找到一条从 A 到 E 的路径。我们想要从这个初始状态开始
达到这个目标状态。
那么我们要怎么做呢？
好吧，我们将从边界开始
包含初始状态。
这将代表我们的边界。
所以我们的边界最初只包含 A，即初始状态
我们要从哪里开始。
现在我们将重复这个过程。
如果边界是空的，则无解。
这不是问题，因为边界不是空的。
因此，我们将从边界中删除一个节点作为下一个要考虑的节点。
边界上只有一个节点。
所以我们将继续将其从边境中删除。
但是现在A，这个初始节点，这就是我们目前考虑的节点。
我们进行下一步。
我们问自己，这个节点是目标吗？
不，这不对。
A 不是目标。
E是目标。
所以我们不返回解决方案。
因此，我们进入最后一步，展开节点
并将结果节点添加到边界。
这意味着什么？
嗯，这意味着采取这个状态 A 并考虑下一步我们可以到达哪里。
在A之后我们接下来能做的只有
B. 这就是我们展开 A 时得到的结果。我们找到了 B。
我们将 B 添加到边界。
现在 B 位于边界，我们再次重复该过程。
我们说，好吧。
边境并不空旷。
因此，让我们从边界中删除 B。
B 现在是我们正在考虑的节点。
我们问自己，B 是目标吗？
不，这不对。
因此，我们继续扩展 B 并将其生成的节点添加到边界。
当我们扩展 B 时会发生什么？
换句话说，我们从B可以到达哪些节点？
好吧，我们可以到达 C 和 D。所以我们继续
并从边界添加 C 和 D。
现在我们在边界有两个节点，C 和 D。
我们再次重复这个过程。
我们从边界中删除一个节点，现在我们将
只需选择 C ​​即可任意执行此操作。
稍后我们将了解为什么如何选择从边界中删除哪个节点
实际上是算法中相当重要的一部分。
但现在我会随意删除C，说这不是目标，
所以我们将添加 E，即边界的下一个。
然后假设我将 E 从边界中删除。
现在我正在查看状态 E。这是目标状态吗？
这是因为我正在寻找一条从A到E的路径。
所以我会回球。
现在，这就是解决方案，我现在就是
能够返回解决方案，我找到了从 A 到 E 的路径。
这就是这个搜索算法的总体思路、总体方法，
按照以下步骤不断从边界删除节点
直到我们能够找到解决方案。
所以你可能会合理地问下一个问题
是，这里会出什么问题吗？
这种方法有哪些潜在问题？
下面是这种方法可能出现的问题的一个示例。
想象一下同一张图，与以前一样，但有一个变化。
现在的变化是，不再只是从 A 到 B 的箭头，
我们还有一个从 B 到 A 的箭头，这意味着我们可以双向移动。
这在 15 谜题中是正确的
当我向右滑动一个图块时，我
然后可以将图块向左滑动以返回到原始位置。
我可以在A和B之间来回。
这就是这些双箭头所象征的意义，即从一个状态
我可以去另一个地方然后再回来。
在许多搜索问题中都是如此。
如果我现在尝试应用相同的方法会发生什么？
好吧，我将从 A 开始，和以前一样。
我会将 A 从边界中删除。
然后我会考虑从A可以到哪里。在A之后，唯一的地方
我可以选择B，这样B就进入了边境。
那我就说，好吧。
我们看一下B，这是边境仅存的东西。
从 B 可以到哪里？
以前只是C和D，但现在因为那个反向箭头，
我可以到达 A 或 C 或 D。所以所有三个 A、C 和 D。所有这些
现在进入边境。
它们是我可以从 B 到达的地方。现在我从边境删除了一个，
而且，你知道，也许我不走运，也许我选了 A。
现在我又看A了。
我考虑从 A 可以去哪里。从 A 可以去 B。
现在我们开始看到问题 如果我不小心
我从A到B，然后回到A，然后再回到B。
我可能会陷入这个我从未经历过的无限循环
取得任何进展，因为我不断地在两者之间来回走动
说我已经看过了。
那么解决这个问题的办法是什么呢？
我们需要一些方法来处理这个问题。
以及我们处理这个问题的方法
是通过某种方式跟踪我们已经探索过的内容。
逻辑是，如果我们已经探索过这个状态，
没有理由回去。
一旦我们探索了一种状态，就不要再回到它，
不必费心将其添加到边界。
没必要。
所以这是我们修改后的方法，
解决此类搜索问题的更好方法。
只需进行一些修改，它看起来就会非常相似。
我们将从包含初始状态的边界开始。
和之前一样。
但现在我们将从另一个数据结构开始，
这只是我们已经探索过的一组节点。
那么我们探索了哪些状态呢？
最初，它是空的。
我们有一个空的探索集。
现在我们重复一遍。
如果边界是空的，则无解。
和之前一样。
我们从边界中删除一个节点，我们检查
要查看它是否是目标状态，请返回解决方案。
到目前为止，这一切都没有什么不同。
但现在我们要做的是
将节点添加到探索状态。
因此，如果碰巧我们从边界中删除一个节点
这不是目标，我们会将其添加到探索集中
这样我们就知道我们已经探索过了。
如果稍后再次出现，我们无需再次返回。
然后最后一步，我们扩展节点
然后我们将结果节点添加到边界。
但在我们总是将结果节点添加到边界之前，
这次我们要聪明一点。
我们只会将节点添加到边界
如果他们还没有到达边境并且如果他们
尚未在探索集中。
所以我们将检查边界和探索集，
确保该节点尚未位于这两个节点之一中，
只要不是，我们就会继续添加到边界
但并非如此。
因此，修改后的方法最终是
有什么可以帮助确保我们不会
在两个节点之间来回移动。
现在我在这里掩盖了一点
到目前为止，这一步是从边界中删除一个节点。
在我随意选择之前，就像我们删除一个节点一样。
但事实证明它实际上非常重要
我们如何决定构建我们的前沿，我们如何添加它们，
以及我们如何删除节点。
边界是一种数据结构。
我们需要选择以什么顺序
我们要删除元素吗？
用于添加和删除元素的最简单的数据结构之一
是一种叫做堆栈的东西。
堆栈是一种后进先出数据类型的数据结构。
这意味着我添加到边界的最后一件事
这将是我从边境移除的第一件事。
所以最近进入堆栈的东西，或者在这种情况下是边界，
将是我探索的节点。
那么让我们看看如果我应用这种基于堆栈的方法会发生什么
 对于这样的问题，找到从 A 到 E 的路径。
将会发生什么？
好吧，我们还是从 A 开始。我们会说，好吧。
我们先来看看A。
然后，请注意，这一次，我们已将 A 添加到探索集中。
A是我们现在已经探索过的东西，我们有这个数据结构
这就是跟踪。
然后我们说从 A 可以到达 B。好吧。
从B我们能做什么？
那么从 B 出发，我们可以探索 B 并到达 C 和 D。
所以我们添加了 C，然后添加了 D。现在当我们探索一个节点时，
我们将把边界视为一个堆栈，后进先出。
D 是最后一个进来的，所以我们接下来将继续探讨这个问题。
然后说，好吧，我们可以从 D 去哪里？
好吧，我们可以到达 F。所以，好吧。
我们将把 F 放入边界。
现在因为边界是一个堆栈，F
是堆栈中最近消失的东西。
所以F就是我们接下来要探索的。
我们将探索 F 并说，好吧。
我们可以从 F 那里得到你吗？
好吧，我们哪儿也去不了，所以边界上什么也没有添加。
那么现在边界中添加的最新内容是什么？
好吧，这不是 C，它是边界中唯一剩下的东西。
我们将对此进行探索，从中我们可以说，好吧，从 C 我们可以到达 E。
所以E进入了边境。
然后我们说，好吧。
让我们看看E，E就是现在的解决方案。
现在我们已经解决了这个问题。
因此，当我们将边界视为堆栈时，
后进先出的数据结构，这就是我们得到的结果。
我们从A到B到D再到F，然后我们倒退并下去
到 C，然后是 E。重要的是要了解如何进行
这个算法正在工作。
我们在这个搜索树中走得很深，所以
说起来，一路走到谷底，我们就走进了死胡同。
然后我们有效地备份并探索了另一条路线
我们之前没有尝试过。
这在搜索树的思想中非常深入，
这样，当我们使用堆栈时，算法最终会起作用，
我们将此版本的算法称为深度优先搜索。
深度优先搜索是搜索算法
我们总是探索前沿最深的节点。
我们不断深入搜索树。
然后，如果我们遇到了死胡同，我们就会后退并尝试其他方法。
但深度优先搜索只是可能的搜索选项之一
我们可以使用。
原来还有另一种算法叫
广度优先搜索，其行为与深度非常相似
第一次搜索有一个区别。
而不是总是以这种方式探索搜索树中最深的节点
深度优先搜索，广度优先搜索
总是要去探索前沿最浅的节点。
那么这是什么意思呢？
嗯，这意味着不使用堆栈，
深度优先搜索（DFS），用于最近添加的项目
边界是我们接下来要探索的，在广度优先搜索（BFS）中，
将改为使用队列，其中队列是先进先出的数据
类型，我们添加到边界的第一个东西是第一个
我们将探索。
他们有效地排成一队或队列，
你越早到达边境，就越早被探索。
那么对于从 A 寻找路径的同样问题意味着什么呢
到E？
好吧，我们从 A 开始，和以前一样。
然后我们继续探索A，然后说，从A我们可以去哪里？
嗯，从A我们可以到达B。和以前一样。
从B开始，和以前一样。
我们可以到达 C 和 D，以便将 C 和 D 添加到边界。
不过，这一次我们将 C 添加到了前沿
在 D 之前，所以我们将首先探索 C。
因此，C 得到了探索。
从C 出发，我们可以去哪里？
好吧，我们可以到达E。
因此 E 被添加到边界。
但因为 D 是在 E 之前探索的，所以接下来我们将讨论 D。
那么我们将探索 D 并说，我们可以从 D 到哪里？
我们可以到达F。只有到那时我们才会说，好吧。
现在我们可以到达 E。那么什么是广度优先搜索，或 BFS，
我们从这里开始，我们查看了 C 和 D，
然后我们看E。实际上我们是
以远离初始状态的方式看待事物，
然后两个远离初始状态。
直到那时，事情才与初始状态相差三分。
与深度优先搜索不同，深度优先搜索只是尽可能深入
进入搜索树，直到遇到死胡同，然后，
最终，不得不备份。
所以现在这是两种不同的搜索算法
我们可以应用它来尝试解决问题。
让我们看看这些实际上是如何实现的
例如，在实践中解决迷宫之类的事情。
这是一个迷宫的例子。
这些空单元格代表我们的代理可以移动的地方。
这些暗灰色的细胞代表墙壁
代理无法通过。
最终，我们的代理，我们的人工智能，将会
尝试找到从位置 A 出发的方法
通过一些动作序列到达位置 B，这些动作被保留在其中，
右、上、下。
在这种情况下深度优先搜索会做什么？
深度优先搜索只会遵循一条路径。
如果它到达一个岔路口，那里有多种不同的选择，
在这种情况下，深度优先搜索只是选择一个。
没有真正的偏好。
但它会继续追随一个，直到陷入死胡同。
当遇到死胡同时，深度优先搜索
有效地返回到最后一个决策点
并尝试另一条路。
完全耗尽这整条路径，当
它意识到，好吧，目标不在这里，
然后它将注意力转向这条路径。
它尽可能深入。
当它遇到死胡同时，它会后退，然后尝试另一条路，
沿着一条特定的道路尽可能深入地走下去，
当它意识到这是一个死胡同时，它就会后退。
然后最终找到实现目标的方法。
也许你很幸运，也许你早些时候做出了不同的选择，
但最终这就是深度优先搜索的工作方式。
它将继续追随，直到陷入死胡同。
当它遇到死胡同时，它会后退并寻找不同的解决方案。
所以你可能会合理地问一件事
是，这个算法总是有效吗？
它总能找到从初始状态到达目标的方法吗？
事实证明，只要我们的迷宫
是有限的，只要它们是有限个空间，其中
我们可以旅行，那么是的。
深度优先搜索将找到解决方案，因为最终它
只会探索一切。
如果迷宫恰好是无限的并且存在无限的状态空间，则
确实存在某些类型的问题，那么情况就略有不同。
但只要我们的迷宫有有限多个方格，
我们会找到解决方案。
不过，我们想问的下一个问题是
是的，这会是一个好的解决方案吗？
这是我们能找到的最优解吗？
答案是不一定。
让我们看一个例子。
例如，在这个迷宫中，我们再次尝试找到从 A 到 B 的路。
您会注意到这里有多种可能的解决方案。
我们可以往这边走，也可以按顺序上去
从 A 到 B。现在如果我们幸运的话
深度优先搜索会选择这种方式到达B。但是没有理由，
必然，为什么深度优先搜索会选择
向上或向右之间。
这是一个任意的决定点，因为两者
将被添加到边界。
最终，如果我们运气不好，深度优先搜索
可能会选择首先探索这条道路，因为它只是
此时的随机选择。
它将探索、探索、探索，最终会
找到目标，这条特定的道路，当现实存在时
是一条更好的道路。
有一个更优化的解决方案，使用更少的步骤，
假设我们根据步骤数来衡量解决方案的成本
我们需要采取的。
所以深度优先搜索，如果我们不幸的话，可能会结束
当有更好的解决方案可用时，却找不到最好的解决方案。
所以如果这就是 DFS，深度优先搜索。
BFS（广度优先搜索）如何比较？
在这种特殊情况下它会如何运作？
那么算法在视觉上看起来会非常不同
就BFS如何探索而言。
因为 BFS 首先查看较浅的节点，
这个想法是 BFS 将首先查看所有节点
距离初始状态还有一距离。
例如，看看这里，看看这里。
就在紧邻该初始状态的两个节点处。
然后它将探索两个相距的节点，
例如，查看状态和状态。
然后它将探索三个相距的节点，即这个状态和那个状态。
而深度优先搜索只是选择了一条路径并继续沿着它走，
另一方面，广度优先搜索是
选择探索所有可能的路径
有点同时，在他们之间反弹，
越来越深入地观察每一个，但
一定要探索较浅的或较浅的
较早接近初始状态。
所以我们将继续遵循这种模式，关注四点之外的事情，
看着五远处的东西，看着
完成六场比赛，直到最终我们到达目标。
在这种情况下，我们确实必须探索一些状态
最终没有带我们去任何地方。
但我们找到的通往目标的路径是最佳路径。
这是我们达到目标的最短路线。
那么，在更大的迷宫中会发生什么呢？
好吧，让我们看一下这样的东西
以及广度优先搜索的行为方式。
好吧，广度优先搜索，再次，将只是
继续关注各州，直到收到决策点。
它可以向左或向右移动。
虽然 DFS 刚刚选择了一个并继续关注
另一方面，BFS 会探索两者，直到陷入死胡同。
它会说，看看这个节点，然后这个节点，
我将查看这个节点，然后查看那个节点，依此类推。
当它到达这里的决策点时，而不是选择左一或右二
并探索这条路径，它将再次探索它们之间的交替，
越来越深。
将探索这里，然后也许在这里和这里，然后继续前进。
探索这里并慢慢前行，你可以直观地看到
看得越来越远。
一旦我们到达这个决策点，我们就会
向上和向下探索，直到最终，我们
让我们朝着目标前进。
你会注意到，是的，广度优先搜索
通过遵循这条特定的路径，我们确实找到了从 A 到 B 的路。
但为了做到这一点，它需要探索很多州。
所以我们在这里看到了 DFS 和 BFS 之间的一些交易。
在 DFS 中，在某些情况下可能会节省一些内存，
与广度优先方法相比，其中
在这种情况下，广度优先搜索必须探索很多状态。
但也许情况并非总是如此。
现在让我们真正将注意力转向一些代码。
看看我们实际上可以的代码
为了实现诸如深度优先搜索或广度之类的东西而编写
例如，用于解决迷宫中的搜索。
所以我将继续进入我的终端。
我在 maze.pi 中拥有的是一个实现
迷宫解决的同样想法。
我定义了一个名为 node 的类，在本例中
正在跟踪状态，即父级，换句话说
状态之前的状态和动作。
在这种情况下，我们不会跟踪路径成本
因为我们可以在最后计算出路径的成本
当我们找到从初始状态到目标的方法之后。
除此之外，我还定义了一个称为堆栈边界的类。
如果不熟悉班级，班级对我来说是一种方式
定义一种在 Python 中生成对象的方法。
它指的是面向对象编程的思想，这里的思想
是我想创建一个对象
能够存储我所有的前沿数据。
我想要有其他已知的功能
作为该对象上的方法，我可以用它来操作该对象。
那么这里发生了什么，如果不熟悉语法，
我有一个最初创建边界的函数
我将使用列表来表示。
最初我的边界由空列表表示。
我的边界一开始就什么都没有。
我有一个添加功能，可以向边界添加一些内容，
就像将其附加到列表的末尾一样。
我有一个函数可以检查边界是否包含特定状态。
我有一个空函数来检查边界是否为空。
如果边界为空，则仅表示边界的长度
为零。
然后我有一个函数可以从边界中删除一些东西。
如果边界是空的，我无法从边界中删除某些东西。
所以我首先检查一下。
但除此之外，如果边界不是空的，
还记得我将这个前沿作为堆栈来实现，
后进先出的数据结构。
这意味着我添加到边界的最后一件事，
换句话说，列表中的最后一件事，
是我应该从该边界删除的项目。
所以您将在这里看到的是我删除了列表的最后一项。
如果你用负数索引一个Python列表，
这将为您提供列表中的最后一项。
由于零是第一项，因此负数
有点环绕并让您到达列表中的最后一项。
所以我们给它节点。
我们称该节点为节点，我们在第 28 行更新边界，表示：
继续删除刚刚从边界删除的节点。
然后我们返回节点作为结果。所以这堂课在这里有效
实现了边界的想法。
它给了我一种向前沿添加东西的方法和方法
以堆栈形式从边界移除某些内容。
为了更好地衡量，我还实现了一个替代版本
相同的东西称为 Q 边界。
您将在此处看到括号中的内容，它继承自堆栈边界，
这意味着它将执行与堆栈边界相同的所有操作，
除了我们从边界删除节点的方式
会略有不同。
不像我们在堆栈中那样从列表末尾删除，
相反，我们将从列表的开头删除。
self.frontierzero 将为我提供边界中的第一个节点，
第一个添加的。
这将是我们在 Q 的情况下返回的内容。
在这里我定义了一个名为迷宫的类。
这将处理获取序列的过程，像迷宫一样的文本
文件，并找出解决方法。
所以我们将把一个看起来像这样的文本文件作为输入
例如，像这样，我们看到代表的散列标记
墙壁和我有代表起始位置的字符 A，
字符B代表结束位置。
您现在可以看一下解析该文本文件的代码。
这是不太有趣的部分。
更有趣的部分是这里的求解函数，
求解函数将在哪里计算出
如何实际从 A 点到达 B 点。
在这里我们看到了完全相同想法的实现
我们从刚才就看到了。
我们将跟踪有多少个州
我们进行了探索，以便稍后可以报告这些数据。
但我从一个仅代表开始状态的节点开始。
我从一个边界开始，在本例中是堆栈边界。
鉴于我将我的边界视为一个堆栈，
您可能会想象我在这里使用的算法现在是深度优先搜索。
因为深度优先搜索或DFS使用堆栈作为其数据结构。
最初，该边界仅包含起始状态。
我们初始化一个最初为空的探索集。
到目前为止我们还没有探索过任何东西。
现在这是我们的循环，即一次又一次重复某事的概念。
首先，我们通过调用我们创建的空函数来检查边界是否为空。
刚才看到了执行情况。
如果边界确实是空的，我们将
继续并引发异常或 Python 错误，表示抱歉。
这个问题没有解决办法。
否则，我们将继续从边界中删除一个节点，
例如通过调用 frontier.remove 并更新我们探索过的状态数量。
因为现在我们已经探索了另一种状态
所以我们说 self.numexplored 加等于一，将状态数加一
我们已经探索过。
一旦我们从边界中删除一个节点，回想一下
下一步是看看它是否是目标，目标测试。
就迷宫而言，目标非常简单。
我检查节点的状态是否等于目标。
最初，当我设置迷宫时，我设置了
这个值称为目标，它是迷宫的属性
所以我可以检查该节点是否确实是目标。
如果这是目标，那么我想要做什么
是回溯我的方式去弄清楚什么行动
我为了达到这个目标而采取了。
我该怎么做？
我们会记得每个节点都存储其父节点——
我们用来到达该节点的之前的节点--
以及为了到达那里而采取的行动。
所以我可以不断地创建这个循环
只需查看每个节点的父节点并保持
为所有家长跟踪我采取了什么行动从家长那里得到
对此。
所以这个循环将不断重复这个遍历所有内容的过程
父节点，直到我们回到初始状态，即
没有父级，其中 node.parent 将等于 none。
当我这样做时，我将建立所有的清单
我正在执行的操作以及所有单元格的列表
这是解决方案的一部分。
但我会扭转它们，因为当我建造它时
从目标回到初始状态，
我正在构建从目标到初始状态的行动序列，
但我想反转它们以获得操作顺序
从初始状态到目标状态。
最终，这将成为解决方案。
因此，如果当前状态等于目标，所有这些都会发生。
否则，如果这不是目标，那么，
然后我将继续将此状态添加到探索集中，
我现在已经探索过这种状态。
如果将来遇到它，就不必再回去了。
然后，这里的逻辑实现了这个想法
将邻居添加到边界。
我是说，看看我所有的邻居。
我实现了一个名为neighbors的函数，你可以看一下。
对于每一个邻居，我都会检查，
该州已经在边境了吗？
该状态是否已在探索集中？
如果它不在其中任何一个中，那么我将继续添加这个新孩子
节点——这个新节点——
到边境。
所以这里有相当多的语法，但关键在这里
并不是要理解语法的所有细微差别，
不过请随意自行仔细查看此文件
了解它是如何工作的。
但关键是看看这是如何实现相同的伪代码，
与我们刚才在屏幕上描述的想法相同
查看我们可能按顺序遵循的步骤
来解决此类搜索问题。
现在让我们实际看看它的实际效果。
例如，我将继续在 maze1.txt 上运行 maze.py。
我们将在这里看到迷宫最初的打印输出
看起来像。
然后，在下面，我们解决了这个问题。
为了做到这一点，我们必须探索 11 个州，并且找到了一条从 A 到 B 的路径。
在这个程序中，我只是碰巧生成了一个图形表示
其中，还有——
这样我就可以打开这个程序生成的maze.png——
它向你显示了墙在哪里，这里的深色颜色是墙的位置。
红色是初始状态，绿色是目标状态，
黄色是所遵循的路径。
我们找到了从初始状态到目标的路径。
但现在让我们看一下更复杂的迷宫
看看会发生什么。
现在让我们看看 maze2.txt，现在我们有一个更大的迷宫。
再次，我们试图找到从 A 点到 B 点的路，
但现在你会想象深度优先搜索可能没那么幸运。
第一次尝试可能无法达到目标。
它可能必须遵循一条路径然后原路返回
稍后再探索其他内容。
那么让我们试试这个。
运行 maze2.txt 的 pythonmaze.py，这次尝试另一个迷宫。
现在深度优先搜索已经能够找到解决方案了。
正如星星所示，这里是从 A 到 B 的一种方式。
我们可以通过打开这个迷宫来直观地表示这一点。
这就是迷宫的样子。
并以黄色突出显示，是从初始状态找到的路径
到目标。
但在找到那条路之前，我们需要探索多少个州呢？
好吧，回想一下，在我的程序中，我一直在跟踪状态的数量
到目前为止我们已经探索过。
这样我就可以回到航站楼并查看，好吧，按顺序
为了解决这个问题，我们必须探索 399 种不同的状态。
事实上，如果我对程序做一点小小的修改
并在最后告诉程序我们何时输出该图像，
我添加了一个名为“显示探索”的参数。
如果我将“显示探索”设置为 true
并通过在 maze2 上运行该程序来重新运行该程序 pythonmaze.py，
然后我打开迷宫，你会看到这里以红色突出显示，
是从初始状态必须探索的所有状态
到目标。
深度优先搜索（DFS）并没有立即找到到达目标的方法。
它选择先探索这个方向。
而当它探索这个方向的时候，它已经
一路走下去，走每一条可以想象的路
直到最后，即使是这又长又曲折的一件事，
为了认识到这一点，你知道吗，这是一条死胡同。
相反，程序需要回溯。
去了这个方向之后，就一定是去了这个方向了。
它在这里很幸运，只是没有选择这条路。
但这里就倒霉了，探索这个方向，探索了一堆状态
它不需要，然后，同样，
探索图表顶部的所有部分
当它可能也不需要这样做时。
总而言之，这里确实是深度优先搜索
表现不佳，或者可能探索超出需要的状态。
它找到最佳解决方案，实现目标的最佳路径，
但为此需要探索的州数量，
我必须采取的步骤数量要多得多。
那么我们来比较一下。
广度优先搜索（BFS）在这个完全相同的迷宫中会如何表现？
为了做到这一点，这是一个非常容易的改变。
DFS 和 BFS 的算法相同，但有不同之处
我们使用什么数据结构来表示边界。
在 DFS 中我使用了堆栈边界——
后进先出 -
而在 BFS 中，我将使用队列边界——首先，
首先，我添加到边界的第一个东西
是我删除的第一件事。
所以我会回到终端，在同一个迷宫上重新运行这个程序，
现在你会看到我们必须探索的州数只有 77 个，
相比之下，当我们使用深度优先搜索时，数量几乎为 400。
我们可以清楚地看到原因。
如果我们现在打开 maze.png 看看，就可以看到发生了什么。
同样，黄色突出显示是呼吸优先搜索找到的解决方案，
顺便说一句，这与深度优先搜索找到的解决方案相同。
他们都在寻找最佳解决方案，但请注意所有未探索的白色区域
细胞。
需要探索的状态要少得多
为了达到目标，因为广度优先搜索起作用
稍微浅一点。
它正在探索接近初始状态的事物
而不探索更远的事物。
所以如果目标不是太远，那么广度优先搜索
实际上可以在迷宫中非常有效地表现
看起来有点像这样。
现在，在这种情况下，BFS 和 DFS 最终都找到了相同的解决方案，
但情况并非总是如此。
事实上，让我们再看一个示例，例如 maze3.txt。
在maze3.txt中，注意这里有多种方法
你可以从 A 到达 B。
这是一个相对较小的迷宫，但让我们看看会发生什么。
如果我使用 - 我会继续并关闭“显示探索”所以
我们只看到解决方案。
如果我使用 BFS（广度优先搜索）来解决 maze3.txt，
好吧，那我们就找到解决办法。
如果我打开迷宫，这就是我们找到的解决方案。
这是最优的。
只需四步，我们就可以从初始状态得到
目标是什么。
但是，如果我们尝试使用深度优先搜索或 DFS，会发生什么情况呢？
好吧，我再次回到我的队列边界，队列边界意味着
我们正在使用广度优先搜索。
我会将其更改为堆栈边界，这意味着现在我们将
使用深度优先搜索。
我将重新运行 Pythonmaze.py。
现在你会看到我们找到了解决方案，
但这不是最佳解决方案。
相反，这就是我们的算法所发现的。
也许深度优先搜索会找到这个解决方案。
这是可能的，但不能保证，如果我们只是
碰巧运气不好，如果我们选择这个状态而不是那个状态，
那么深度优先搜索可能会找到一条更长的路线来获得
从初始状态到目标状态。
所以我们确实在这里看到了一些深度优先搜索可能不会的权衡
找到最优解。
所以从这一点来看，广度优先搜索似乎相当不错。
这是我们能做的最好的事情吗？它将为我们找到最佳解决方案
我们不必担心以下情况
我们最终可能会找到比实际存在的解决方案更长的路径？
目标距离初始状态还很远——
我们可能需要采取很多步骤才能从初始状态恢复过来
向目标——
最终发生的事情是，这个算法，BFS，最终
基本上探索整个图，必须穿过整个迷宫
以便找到从初始状态到目标状态的路径。
我们最终想要的是我们的算法
变得更聪明一点。
现在这对我们的算法意味着什么
在这种情况下，要变得更聪明一点吗？
好吧，让我们回顾一下广度优先搜索可能会在哪里
能够做出不同的决定
并在此过程中考虑人类的直觉。
比如，当人类解决这个迷宫时，人类会做什么？
BFS最终选择做什么？
嗯，BFS 做出的第一个决策点
就在这里，当它走了五步并结束时
处于一个岔路口的位置。
它可以向左走，也可以向右走。
在最初的几个步骤中，我们别无选择。
每个州只能采取一项行动。
所以搜索算法做了唯一的事情
任何搜索算法都可以做到
是在下一个状态之后继续遵循该状态。
但这个决策点是事情变得有点有趣的地方。
深度优先搜索，我们看到的第一个搜索算法，
选择说，让我们选择一条路径并穷尽那条路径，
看看这样的方式是否有目标，如果没有，那么让我们
尝试其他方式。
广度优先搜索采用了另一种方法：
你知道吗？
我们先探讨一下浅层的东西，先离我们较近的，左看看右看看，
然后向左返回，向右返回，依此类推，
我们交替选择，希望能找到附近的东西。
但最终，如果面对这样的情况，人类会做什么
向左走还是向右走？
好吧，人类可能会在视觉上看到这一点，好吧，
我正试图到达状态 B，就在上面，然后向右走
感觉离目标又近了一步。
就像，感觉应该是向右走
比向左走更好，因为我正在进步
实现这一目标。
当然，现在我在这里做出一些假设。
我假设我们可以代表
这个网格就像一个二维网格，
我知道一切的坐标。
我知道 A 在坐标 0,0 中，B 在其他坐标对中。
我知道我现在在什么坐标，所以我可以计算出，是的，去
这样，就离目标更近了。
对于某些类型的搜索来说，这可能是一个合理的假设
问题，但在其他方面可能没有。
但现在，我们将继续假设——
我知道我当前的坐标对，并且我知道坐标 x,y
我想要达到的目标。
在这种情况下，我想要一个算法
稍微聪明一点并且知道
我应该朝着目标取得进展，
这可能就是做到这一点的方法，因为在迷宫中，
沿目标坐标方向移动
通常（尽管并不总是）是一件好事。
所以在这里我们区分两种不同类型的搜索
算法——无信息搜索和有信息搜索。
无信息搜索算法是像DFS和BFS这样的算法，
我们刚刚看到的两种算法，
这是不使用任何问题特定知识的搜索策略
才能解决问题。
DFS 和 BFS 并不真正关心结构
迷宫的信息或任何有关迷宫排列方式的信息
来解决问题。
他们只是查看可用的操作并从这些操作中进行选择，
无论是迷宫还是其他问题都没关系。
解决方案，或者尝试解决问题的方式，
本质上真的是一样的。
我们现在要看看的是
对无信息搜索的改进。
我们将看一下知情搜索。
知情搜索将成为搜索策略
使用特定于问题的知识能够更好地找到解决方案。
而在迷宫的情况下，这个问题的具体知识
就像，如果我要平方
地理位置上更接近目标，即
比在地理上更远的广场更好。
这是我们只有思考这个问题才能知道的
并推理哪些知识可能对我们的人工智能代理有帮助
了解一些有关的事情。
有许多不同类型的知情搜索。
 具体来说，首先，我们要看看一个特定的类型
的搜索算法称为贪婪最佳优先搜索。
贪婪最佳优先搜索，通常缩写为 GBFS，
是一种搜索算法，它不是扩展最深的节点，
比如DFS，或者最浅的节点，比如BFS，
这个算法总是会扩展节点
它认为最接近目标。
现在，搜索算法无法确定它是否是最接近的
目标的事情，因为如果我们知道什么最接近目标
一直这样，那么我们就已经有了解决方案。
比如，了解什么接近目标，
我们可以按照这些步骤从初始位置开始
到解决方案。
但如果我们不知道解决方案——也就是说我们不确切知道
什么最接近目标——
相反，我们可以使用对什么的估计
最接近目标，也称为启发式——
只是评估我们是否接近目标的某种方法。
我们将使用启发式函数来实现这一点，通常称为 h(n)，
它接受输入状态并返回我们对我们的接近程度的估计
正在达到目标。
那么这个启发式函数实际上可能是什么
看起来像迷宫求解算法吗？
当我们试图解决迷宫问题时，启发式是什么样的？
好吧，启发式需要回答一个问题，比如在这两个问题之间
C和D细胞哪个更好？
如果我想找到实现目标的方法，我更愿意选择哪一种？
好吧，任何人都可能看到这个并告诉你，你知道吗？
D 看起来更好。
即使迷宫错综复杂，你也没有考虑过所有的墙壁，
D 可能更好。
为什么D更好？
好吧，因为如果你忽略墙壁——让我们假装墙壁
暂时不存在并放松问题，可以这么说——
D，仅就坐标对而言，更接近这个目标。
实现目标所需的步骤更少，
与C相比，即使你忽略墙壁。
如果你只知道 C 的 x,y 坐标和球门的 x,y 坐标，
同样，你知道 D 的 x,y 坐标，
你可以计算出D，只是在地理上，忽略墙壁，
看起来好一些了。
这就是我们要使用的启发式函数，
这就是所谓的曼哈顿距离，一种特定类型
启发式的，启发式在哪里，垂直有多少个正方形
水平然后从左到右——所以不是
允许自己沿对角线走，要么向上，要么向右，要么向左，要么向下。
我需要采取多少步骤才能从每个单元格到达目标？
嗯，事实证明，D 更接近。
步骤较少。
只需要采取六个步骤即可实现这一目标。
再次忽略墙壁。
我们已经稍微放松了这个问题。
我们只是关心 如果你算一下
将 x 值与 y 值相减
彼此的价值观，我们估计我们的距离有多远？
我们可以估计D比C更接近目标。
现在我们有了一个方法。
我们有一种方法可以选择从边界中删除哪个节点。
在我们算法的每个阶段，我们
将从边界删除一个节点。
我们将探索该节点，如果它具有最小的
此启发式函数的值（如果它具有最小值）
曼哈顿距球门的距离。
那么这实际上会是什么样子呢？
好吧，让我首先标记这个图，标记这个迷宫，
一个代表该启发式值的数字
函数，距任何这些像元的曼哈顿距离值。
例如，从这个牢房开始，我们就远离了目标。
距离这个牢房还有两点距离。
三客，四客。
在这里，我们还差五个，因为我们必须向右走一处，然后向上走四处。
从这里开始，曼哈顿距离为 2。
我们距离球门只有两格了
从地理上看，尽管在实践中我们
必须走更长的路，但我们还不知道。
启发式只是一些简单的估计方法
我们离目标还有多远。
也许我们的启发过于乐观了。
它认为，是的，我们只有两步之遥，
在实践中，当你考虑墙壁时，可能会需要更多的步骤。
所以这里重要的是启发式方法并不能保证
将采取多少步。
它正在估计。
这是一种试图近似的尝试。
通常看起来确实如此，看起来更近的方块
启发式函数的目标值较小
比距离更远的方块。
那么现在，使用贪婪的最佳优先搜索，该算法实际上可以做什么？
好吧，再说一遍，对于前五个步骤，没有太多选择。
我们开始这个初始状态，A。我们说，好吧。
我们必须探索这五种状态。
但现在我们有一个决策点。
现在我们可以选择向左走还是向右走。
之前，DFS 和 BFS 会任意选择，因为它只是
取决于你将这两个节点放入边界的顺序--
我们没有指定您将它们放入边境的顺序，仅指定了顺序
你把它们拿出来。
在这里我们可以看看 13 和 11 并说，好吧，
这个方格距球门的距离是11，
根据我们的启发，根据我们的估计。
而这一场我们估计距离目标还差 13 分。
所以在这两个选择之间，在这两个选择之间，
我宁愿要11号
我宁愿距球门 11 步，所以我会向右走。
我们能够做出明智的决定，因为我们了解更多
关于这个问题。
那么我们就继续关注10、9、8——
两个七人制之间。
我们确实没有太多方法来了解它们。
那么我们就只能做出任意的选择。
你知道吗？
也许我们选择错了。
但这没关系，因为现在我们仍然可以说，好吧，让我们尝试一下这七个。
我们说七、六。
即使它增加了，我们也必须做出这个选择
启发式函数的值。
但现在我们在六到八之间有另一个决策点。
而在这两者之间——
事实上，我们也在考虑 13 个，但这个数字要高得多。
六、八、十三之间，嗯，六
是最小值，所以我们宁愿取 6。
我们能够做出明智的决定，走这条路到右边
可能比走那条路更好。
所以我们就往这边转。
我们去五点。
现在我们找到一个决策点，我们实际上将
做出我们可能不想做出的决定，
但不幸的是，没有太多办法解决这个问题。
我们看到四个和六个。
四看起来离目标更近了，对吧？
还在往上走，目标也更高了。
所以我们最终走上了这条路，最终把我们带进了死胡同。
但这没关系，因为我们仍然可以说，好吧，现在让我们尝试六个，
现在就沿着这条最终引导我们实现目标的路线前进。
所以现在这就是贪婪的最佳优先搜索可能会发生的情况
尝试解决这个问题，通过说每当
我们在可以探索的多个节点之间做出了决定，
让我们探索 h(n) 值最小的节点，
这个启发式函数正在估计我还需要走多远。
碰巧的是，在这种情况下，
就我们需要探索的状态数量而言，我们最终做得更好，
比 BFS 需要的多。
BFS 探索了本节和该节的所有内容。
但我们能够通过利用
这种启发式的知识，关于我们有多接近
是为了实现该想法的目标或某种估计。
所以这看起来好多了。
那么我们不是总是更喜欢算法吗
像这样而不是像广度优先搜索这样的算法？
也许。
需要考虑的一件事是我们
需要想出一个好的启发法。
启发式方法的好坏将影响该算法的好坏。
提出一个好的启发式方法常常具有挑战性。
但另一件事要考虑的是问
这个问题，就像我们对前两种算法所做的那样，
这个算法是最优的吗？
它总是能找到从初始状态到目标的最短路径吗？
为了回答这个问题，让我们看一下这个例子。
看一下这个例子。
再一次，我们试图从 A 到 B，再一次，我已经
用曼哈顿距离标记每个单元格
从球门开始，向上和向右的方格数
你需要旅行才能从那个广场到达目标。
让我们考虑一下，贪婪的最佳优先搜索
总是选择最小的数字最终找到最佳解决方案？
最短的解决方案是什么？这个算法能找到它吗？
需要意识到的重要一点是，这就是决策点。
我们估计距离目标还有 12 分。
我们有两个选择。
我们可以往左边走，我们估计离球门有13远，
或者我们可以向上走，我们估计距离目标还有 11 个距离。
在这两者之间，贪婪的最佳优先搜索会说，
11号比13号好看。
这样做时，贪婪的最佳优先搜索
最终会找到这条通往目标的道路。
但事实证明这条路并不是最优的。
有一种方法可以用更少的步骤达到目标。
实际上就是这样，这样最终涉及的步骤更少，
尽管此时此刻意味着选择最坏的
两者之间的选择——或者我们估计是最糟糕的选择，基于
关于异端。
这就是我们所说的贪婪算法的意思。
它正在本地做出最佳决策。
在这个决策点，看起来更好
去这里比去13.
但从大局来看，这并不一定是最优的，
它可能会在现实中找到解决方案
是一个更好的解决方案。
所以我们想要一些方法来解决这个问题。
我们喜欢这种启发式的想法，即
能够估计路径，我们和目标之间的距离，
这有助于我们做出更好的决定
并无需搜索该州的整个地区
空间。
但我们想修改算法以便我们可以实现
最优性，从而使其达到最优。
那么有什么方法可以做到这一点呢？
这里的直觉是什么？
好吧，我们来看看这个问题。
在这个初始问题中，贪婪的最佳优先搜索
在这里找到了这个解决方案，这条漫长的道路。
它之所以不是很好是因为，是的，启发式数字
跌得很低，但后来，他们开始反弹。
他们重新构建了 8、9、10、11——在本例中一直到 12。
那么我们该如何改进这个算法呢？
好吧，我们可能意识到的一件事是，如果我们
一直通过这个算法，通过这条路径，
我们最终走到了 12 点，我们必须采取很多步骤——比如，
谁知道要走多少步——才能到达这 12 步，
作为替代方案，我们也可以采取更少的步骤，只需六个步骤，
并最终在这里 13 。
是的，13 比 12 多，所以看起来不太好，
但它需要的步骤要少得多。
正确的？
只需 6 步即可达到这 13 步，而需要更多步骤
达到这个12.
虽然贪婪的最佳优先搜索说，哦，好吧，12 比 13 好
所以选择 12，我们可能会更明智地说，
我宁愿去某个启发式的地方
如果我能更快地到达那里，看起来需要的时间会稍微长一些。
我们将编码这个想法，这个总体想法，
进入一种更正式的算法，称为 A 星搜索。
明星搜索将通过以下方式解决这个问题：
而不是仅仅考虑启发式，
还要考虑我们到达某个特定状态需要多长时间。
所以区别是贪婪的最佳优先搜索，如果我处于某种状态
现在我唯一关心的是
我之间的估计距离（启发值）是多少
和目标。
而明星搜索将考虑
两条信息。
它会考虑到我估计离目标还有多远，
还有我要走多远才能到达这里？
因为这也相关。
因此，我们将通过扩展具有最低值的节点来搜索算法
g(n) 加上 h(n) 的值。
h(n) 与我们刚才讨论的启发式相同
根据问题的不同而有所不同，但 g(n) 将是达到目标的成本
节点——
在这种情况下，我需要采取多少步才能达到目前的位置。
那么该搜索算法在实践中是什么样的呢？
好吧，让我们看一下。
我们又遇到了同样的迷宫。
我再次用曼哈顿距离标记它们。
该值是 h(n) 值，即启发式估计
每个方格距目标有多远。
但现在，当我们开始探索状态时，我们
不仅关心这个启发值，而且还关心
关于 g(n)，我必须采取的步数才能到达那里。
我关心的是把这两个数字加在一起。
那看起来是什么样子的呢？
在这第一步中，我已经迈出了一步。
而现在我估计距离目标还有16步。
所以这里的总价值是 17。
然后我又迈出一步。
我现在已经采取了两步。
我估计自己距离目标还差 15——
再次，总价值为 17。
现在我已经采取了三步。
我估计距离目标还有 14 秒，依此类推。
四个步骤，估计为 13。
五个步骤，估计为 12。
现在，这是一个决策点。
如果启发式为 13，我可能距离目标还有 6 步
总共 19 步，否则我可能还差六步
启发式为 11 的目标，估计总数为 17。
所以在 19 到 17 之间，我宁愿选择 17——
6加11。
到目前为止，与我们之前看到的没有什么不同。
我们仍然采用这个选项，因为它看起来更好。
我一直选择这个选项，因为它看起来更好。
但正是在这里，事情变得有点不同。
现在我距离目标可能有 15 步，估计距离为 6。
所以 15 加 6，总价值为 21。
或者，我可能距离目标还有六步——
因为这是五步远，所以这是六步远——
据我估计，总价值为13。
所以 6 加 13——
那是 19。
所以这里我们将 g(n) 加上 h(n) 计算为 19——
6 加 13——而在这里，我们将是 15 加 6，即 21。
所以直觉是，19 比 21 少，选择这里。
但最终的想法是我宁愿采取更少的步骤来达到 13
比走了 15 步并处于 6 级
因为这意味着我必须采取更多步骤才能到达那里。
也许这条路有更好的路。
因此，我们将探索这条路线。
现在，如果我们再走一步——这是七步加十四步，
21 岁，所以这两者之间有点难以抉择。
无论如何，我们最终可能会探索这一点。
但之后，随着这些数字在启发值中开始变大
这些启发值开始变小，
你会发现我们实际上会沿着这条路继续探索。
你可以通过数学计算来了解每个决策点，
明星搜索会根据步骤的总和来做出选择
我花了很多时间才达到目前的位置，然后
我估计我离目标还有多远。
因此，虽然我们确实必须探索其中一些状态，
事实上，我们找到的最终解决方案是最优解决方案。
它确实为我们找到了摆脱初始状态的最快方法
到目标。
事实证明，A*是一定条件下的最优搜索算法
状况。
所以条件是 h of n，我的启发式，需要被接受。
启发式的可接受性意味着什么？
好吧，如果启发式永远不会高估真实成本，那么它就是可接受的。
每个事件总是需要要么完全正确
就我而言距离有多远，否则就需要低估。
所以我们看到了之前的一个例子，其中启发值要小得多
比实际花费的成本。
那完全没问题。
但启发价值永远不应该被高估。
它永远不应该认为我离目标比实际距离更远。
同时，为了做出更有力的声明，h of n
也需要保持一致。
保持一致意味着什么？
从数学上来说，这意味着对于每个节点，
我们将称n为后继节点，即我之后的节点，我将称n为素数，
其中执行该步骤需要花费 c 的成本，n 的启发值
需要小于或等于启发式
n素数的值加上成本。
所以这涉及很多数学，但用语言来说，它最终是什么
意思是如果我现在处于这种状态，
从我到目标的启发值不应超过启发值
我的继任者的价值，我可以去的下一个地方，加上多少
只是迈出这一步，从一步到下一步，我就付出了代价。
所以这只是确保我的启发式在所有
我可能会采取的这些步骤。
所以只要这是真的，那么 A* 搜索就会找到一个最优的
解决方案。
这就是解决这些搜索问题的大部分挑战所在
有时会出现，A* 搜索是一种已知的算法，
你可以很容易地编写代码。
但选择启发式方法才是有趣的挑战。
启发式越好，我就越好
能够解决问题，并且我需要探索的状态也更少。
我需要确保启发式满足
这些特殊的限制。
总而言之，这些是搜索算法的一些示例
这可能有用。
当然，还有更多。
例如，A* 确实倾向于使用相当多的内存，
因此 A* 有其他替代方法，最终使用的内存比
这个版本的A*恰好使用。
还有其他针对其他情况进行优化的搜索算法
以及。
但到目前为止，我们只关注搜索算法
那里只有一名代理人。
我正在努力寻找问题的解决方案。
我正试图在迷宫中寻找出路。
我正在尝试解决 15 个难题。
我正在尝试查找从 A 点到 B 点的行车路线。
但有时在搜索情况下，我们会
进入我所处的敌对局面
试图做出明智决策的代理人，
可以这么说，还有其他人在与我作战，
有着相反目标的人，我正在努力取得成功的人，
另一个希望我失败的人。
这在游戏中最流行，像井字棋这样的游戏，
我们有这个 3×3 网格，以及 X 和 O
轮流在这些方格中的任一方格中写下 X 或 O。
目标是连续获得三个 X，如果你是 X 玩家，
或者连续三个 O（如果您是 O 玩家）。
计算机已经非常擅长玩游戏了，井字棋很容易，
但甚至更复杂的游戏。
所以你可能会想象，游戏中的明智决策是什么样的
喜欢？
所以也许 X 在中间先下棋，O 在这里下棋。
X 的智能举措现在变成了什么？
如果你是X，你应该搬到哪里？
事实证明有几种可能性。
但如果人工智能以最佳方式玩这个游戏，
那么人工智能可能会在右上角这样的地方玩，
在这种情况下，O 的目标与 X 相反。
X 正在努力赢得比赛，在对角线上取得三连胜，
O 正试图阻止该目标，但与目标相反。
所以 O 将放在这里，试图阻止。
但现在，X 有一个非常聪明的举动。
X 可以采取行动，就像这样，现在 X 有两种可能的方式
可以赢得比赛。
X 可以通过在这里连续获得三个来赢得比赛，
或者 X 可以通过这种方式垂直连续三个来赢得比赛。
因此，O 的下一步行动并不重要。
O可以在这里打，例如水平阻挡三排，
但随后 X 将通过垂直三连胜来赢得比赛。
所以有相当多的推理
这是为了让计算机能够解决问题而进行的。
它在本质上与我们迄今为止所研究的问题相似。
有行动，有董事会的某种状态，
以及从一个动作到下一个动作的一些过渡，
但不同之处在于，现在这不仅仅是经典搜索
问题，而是一个对抗性搜索问题，我是 X 玩家，
试图找到最好的行动，但我
知道有一些对手试图阻止我。
所以我们需要某种算法来处理这些对抗类型
的搜索情况。
我们要看看的算法
是一种称为 Minimax 的算法，它的工作原理
对于这些有两个玩家的确定性游戏来说非常好。
它也适用于其他类型的游戏，但我们现在看看游戏
当我采取行动时，我的对手也采取行动，而我正在努力获胜，
我的对手也正在努力获胜。
或者换句话说，我的对手试图让我输。
那么我们需要什么才能使这个算法发挥作用呢？
好吧，每当我们尝试翻译这个人类玩游戏的概念时，
输赢，对于计算机来说，我们想要
将其翻译成计算机可以理解的形式。
最终，计算机实际上只是理解数字。
所以我们需要某种方式来翻译 X 和 O 的游戏
网格上的数字，计算机可以理解的东西。
计算机通常不理解输赢的概念
但它确实理解更大和更小的概念。
所以我们可能要做的是，我们可能会采取每一种可能的方式
井字游戏可以展开并分配一个值或效用，
对于每一种可能的方式。
在井字游戏以及许多类型的游戏中
存在三种可能的结果。
结果是，O 获胜，X 获胜，或者无人获胜。
因此，玩家一获胜，玩家二获胜，或者没有人获胜。
现在，让我们继续分配这些可能的结果
不同的值。
我们会说 O 获胜——
其值为负 1。
没有人获胜——其值为 0。
X 获胜——其值为 1。
因此，我们刚刚为这三种可能的结果分配了数字。
现在，我们有两名球员。
我们有 X 球员和 O 球员。
我们将继续将 X 玩家称为最大玩家。
我们将 O 玩家称为最小玩家。
原因是因为在 Minimax 算法中，
最大玩家（在本例中为 X）的目标是最大化得分。
这些是分数的可能选项：负 1、0 和 1。
X想要最大化分数，这意味着如果可能的话，
X 希望出现 X 赢得比赛的情况。
我们给它打分 1。
但如果这是不可能的，如果X需要在这两者之间做出选择
选项，负1表示O获胜，或0表示无人获胜，
X 宁愿没有人获胜，得分为 0，也不愿得分为负 1，
哦，胜利了。
所以这种及时输赢的概念
在数学上已简化为“尝试并最大化”的想法
分数。
X玩家总是希望分数更大。
另一方面，最小玩家，在这种情况下，O，
旨在最小化分数。
O 玩家希望分数尽可能小。
现在我们已经开始了这场 X 和 O 以及输赢的游戏
然后把它变成数学的东西
其中 X 试图最大化分数，O 试图最小化分数。
现在让我们看看游戏的所有部分
我们需要将其编码到人工智能中
这样人工智能就可以玩井字棋之类的游戏。
所以游戏需要一些东西。
我们需要某种初始状态，在本例中我们需要
调用 S0，这就是游戏开始的方式，就像一个空的井字棋盘，
例如。
我们还需要一个名为player的函数，
玩家函数将把状态作为输入，这里表示
通过 S。玩家函数的输出将是，
现在轮到哪位玩家了？
我们需要能够给计算机提供一个井字棋盘，
通过一个函数运行它，该函数告诉我们轮到谁了。
我们需要一些我们可以采取的行动的概念。
稍后我们就会看到这样的例子。
我们需要一些过渡模型的概念——和以前一样。
如果我有一个状态，并且我采取行动，我
需要知道其结果是什么。
我需要某种方式知道游戏何时结束。
所以这相当于一种目标测试，
但我需要一些终端测试，一些方法来检查
查看状态是否为最终状态，其中最终状态意味着
游戏结束了。
在经典的井字游戏中，终止状态意味着某人
连续获得三个，或者井字棋盘上的所有方格都已
填充。
这些条件中的任何一个都使其成为最终状态。
在国际象棋游戏中，可能是这样的，
当将死时，或者不再可能将死时，
这成为最终状态。
最后我们需要一个实用函数，一个接受状态的函数
并为我们提供该最终状态的数值，换句话说，
如果 X 赢得了比赛，则其值为 1。
如果 O 赢得了比赛，则其值为负 1。
如果没有人赢得比赛，则值为 0。
让我们依次看一下这些内容。
初始状态，我们可以在井字游戏中将其表示为空的游戏板。
这就是我们开始的地方。
这是我们开始搜索的地方。
再次，我将用视觉方式来表现这些东西。
但你可以想象这真的只是
所有这些可能的正方形的数组或二维数组。
然后我们需要播放器函数，它再次需要一个状态
并告诉我们轮到谁了。
假设 X 迈出了第一步，如果我有一个空的游戏板，
那么我的播放器函数将返回 X
如果我有一个游戏板，其中 X 已采取行动，那么我的玩家功能是
将返回 O。玩家函数采用井字棋游戏板
并告诉我们轮到谁了。
接下来，我们将考虑 actions 函数。
actions 函数，很像经典搜索中的那样，采用一个状态
并给我们所有可能的行动的集合
我们可以接受那种状态。
让我们想象一下，轮到 O 在如下所示的游戏盘中移动。
当我们将它传递给 actions 函数时会发生什么？
所以 actions 函数将游戏的这个状态作为输入，
输出是一组可能的动作，它是一组——
我可以在左上角移动，也可以在底部中间移动。
这是我有两种可能的行动选择
当我开始处于这种特殊状态时。
现在，就像以前一样，当我们添加状态和动作时，
我们需要某种过渡模型来告诉我们，
当我们在状态中采取此操作时，我们得到的新状态是什么？
在这里，我们使用结果函数来定义
作为输入的状态以及动作。
当我们将结果函数应用到这个状态时，
说让 O 在左上角移动，我们得到的新状态
是结果状态，其中 O 位于左上角。
现在，对于懂得玩井字游戏的人来说，这似乎是显而易见的。
当然，你玩的是左上角——
这就是你得到的董事会。
但所有这些信息都需要编码到人工智能中。
AI 不知道怎么玩井字棋
直到你告诉人工智能井字游戏的规则是如何运作的。
这个函数，在这里定义函数，
允许我们告诉人工智能这个游戏实际上是如何运作的
以及行动如何实际影响游戏的结果。
所以人工智能需要知道游戏是如何运作的。
人工智能还需要知道游戏何时结束。
那是通过定义一个名为terminal的函数作为输入
状态 S，这样如果我们进行一场尚未结束的比赛，
将其传递到终端函数中，输出为 false。
比赛还没有结束。
但如果我们认为一场比赛已经结束了，因为 X 已经得到了 3 个
沿着对角线排成一行，将其传递到终端函数中，
那么输出将为 true，因为游戏现在实际上已经结束了。
最后，我们告诉人工智能游戏是如何运作的
就可以采取哪些行动以及采取这些行动时会发生什么而言。
我们已经告诉人工智能游戏何时结束。
现在我们需要告诉人工智能每个状态的值是什么。
我们通过定义这个效用函数来做到这一点，它需要一个状态 S，
并告诉我们该状态的分数或效用。
所以我们再说一遍，如果 X 赢得了比赛，那么该效用值为 1，
而如果 O 赢得了比赛，那么其效用为负 1。
人工智能需要知道每一个最终状态
游戏在哪里结束，那个状态有什么用处？
所以我可以给你一个像这样的游戏板，游戏实际上已经结束了，
我要求人工智能告诉我该状态的价值是什么，它可以这样做。
状态值为1。
然而，事情变得有趣的是，如果游戏还没有结束。
让我们想象一下这样的游戏板。
我们正处于游戏中间。
轮到O出手了。
那么我们怎么知道轮到O 采取行动了呢？
我们可以使用播放器函数来计算。
可以说，S的玩家，处于pass状态。
O 是答案，所以我们知道轮到 O 移动了。
现在，这个董事会的价值是什么？O 应该采取什么行动？
嗯，这要看情况。
这里我们必须做一些计算。
这就是极小极大算法真正发挥作用的地方。
回想一下，X 试图最大化分数，这意味着
O 试图最小化分数。
O 希望最小化我们在游戏结束时获得的总价值。
因为这场比赛还没有结束，所以我们并没有真正
还不知道这个游戏板的价值是什么。
我们必须做一些计算才能弄清楚这一点。
那么我们如何进行这样的计算呢？
嗯，为了做到这一点，我们要考虑，
就像我们在经典搜索情况下一样，
 接下来可能会发生什么行动，这将把我们带到什么状态？
事实证明，在这个位置上，有
只有两个开放的正方形，这意味着只有两个开放的地方
O可以采取行动。
O 可以在左上角移动，
或者O可以在下中路移动。
Minimax 并不立即知道这些动作中的哪一个
会更好，所以会考虑两者。
但现在我们遇到了同样的情况。
现在我还有两个游戏板，两个都还没有结束。
接下来发生什么？
从这个意义上说，Minimax 是
我们称之为递归算法。
现在它将重复完全相同的过程，尽管现在
从相反的角度考虑。
就好像我现在要把自己——如果我是 O 玩家，
我会把自己放在对手的立场上，我的对手是X球员，
并考虑一下，如果我的对手处于这个位置，他们会怎么做？
如果我的对手（X 玩家）处于那个位置，他们会怎么做？
然后会发生什么？
好吧，另一个玩家，我的对手，X 玩家，
正在尝试最大化分数，而我正在尝试
以最小化 O 玩家的得分。
所以X试图找到他们可以获得的最大可能值。
那么会发生什么呢？
那么，从这个棋盘位置来看，X只有一个选择。
X将在这里比赛，他们将取得三连胜。
我们知道那个董事会，X 获胜——
其值为 1。
如果 X 赢得了游戏，则该游戏板的值为 1。
所以从这个位置来看，如果这个状态只能导致这个状态，
这是唯一可能的选择，并且该状态的值为 1，
那么 X 玩家可以从这个游戏盘中获得的最大可能值
从这里开始也是 1。
我们唯一能得到的地方就是值为1的游戏，
所以这个游戏板的值为 1。
现在我们在这里考虑这个。
现在会发生什么？
好吧，X 需要采取行动。
X 唯一可以移动的位置是左上角，所以 X 会去那里。
而在这场比赛中，没有人能赢得比赛。
没有人连续三个。
所以该游戏板的值为 0。
没人赢。
同样，按照同样的逻辑，如果从这个董事会位置来看，唯一的地方
我们可以得到一个值为 0 的棋盘，
那么这个状态的值也必须为 0。
现在到了选择部分，即尝试最小化的想法。
我，作为O玩家，现在知道，如果我做出这个选择，
向左上角移动，这将导致游戏值为 1，
假设每个人都发挥最佳水平。
如果我转而踢中下位置，
选择这个岔路口，这将导致一个游戏板
值为 0。
我有两个选择。
我有一个 1 和一个 0 可供选择，我需要选择。
作为最小玩家，我宁愿选择这个选项
与最小值。
所以每当玩家有多种选择时
最小的玩家将选择具有最小值的选项。
最大玩家将选择价值最大的选项。
1和0之间，0更小，
这意味着我宁愿打平也不愿输掉比赛。
所以我们会说这个游戏板的值为 0，
因为如果我打得最好，我就会选择这个岔路口。
我会把我的 O 放在这里来连续阻挡 X 的三个。
X会移动到左上角，游戏就结束了，
没有人会赢得这场比赛。
所以这就是 Minimax 的逻辑，考虑所有可能的选项
我可以采取的所有行动，
然后设身处地为对手着想。
我通过考虑什么行动来决定我现在要采取什么行动
我的对手将在下一个回合做出。
为此，我会考虑之后在转牌圈我会采取什么行动，
依此类推，直到游戏结束，
到这些所谓的终端状态之一。
事实上，就在这个决定点，我
作为 O 玩家，我正在尝试决定要做出什么决定，
可能只是逻辑的一部分，X 玩家，我的对手，
在我面前使用了这个动作。
这可能是一些更大的树的一部分
X 正在尝试在这种情况下采取行动
并需要在三个不同的选项之间进行选择
以便做出关于要发生什么的决定。
我们距离比赛的结束也越来越远，
这棵树必须走得越深，因为这棵树的每一层
将对应于我采取的一个举动、一个举动或行动，
我的对手为了决定发生什么而采取的一项行动或行动。
事实上，事实证明，如果我是这个位置上的X玩家，
我递归地进行逻辑分析，发现我有一个选择——
三个选择，事实上，其中一个导致值为0，如果我在这里玩，
如果每个人都发挥最佳，比赛就会平局。
如果我在这里打球，那么 O 会赢，而我会输，发挥最佳状态。
或者在这里，我，X玩家，可以获胜——
好吧，在 0 分和负 1 分和 1 分之间，
我宁愿选择值为 1 的棋盘，
因为这是我能得到的最大价值。
因此该板的最大值也为 1。
所以这棵树可以变得非常非常深，
尤其是当游戏开始有越来越多的动作时。
这个逻辑不仅适用于井字棋，
但在任何这类游戏中，我采取行动，我的对手采取行动，
最终，我们有这些敌对的目标。
我们可以将图简化为如下所示的图。
这是 Minimax 树的更抽象版本，
这些是每个状态，但我不再准确地表示它们
就像井字游戏板一样。
这只是代表一些可能是井字游戏的通用游戏，
可能完全是其他游戏。
这些绿色箭头中的任何一个向上--
代表最大化状态。
我希望分数尽可能大。
任何一个向下的红色箭头——
这些是最小化状态，其中玩家是最小玩家，
他们正在努力让分数尽可能小。
所以如果你想象在这种情况下，我是最大化玩家，这个玩家
在这里，我有三个选择——
一项选择给我 5 分，一项选择给我 3 分，
其中一个选项给了我 9 分。
那么，在这三个选择中，我最好的选择
就是在这里选择这个9，这个分数使我的选择最大化
所有三个选项中。
所以我可以给这个状态一个值 9，
因为在我的三个选择中，这是最好的
我可以选择的。
这就是我现在的决定。
你想象一下，这就像距离游戏结束仅一步之遥。
但你也可以问一个合理的问题。
距离比赛结束还有两步，我的对手可能会做什么？
我的对手是最小化玩家。
他们正在努力让比分尽可能小。
想象一下，如果他们必须做出哪个选择，会发生什么。
一个选择导致我们进入这种状态，我，最大化的玩家，
我会选择 9，这是我能得到的最大分数。
一个人会导致这种状态，在这种状态下，我，最大化的玩家，
我会选择8，这是我能得到的最大分数。
现在，最小化玩家被迫在 9 或 8 之间做出选择，
将选择尽可能小的分数，在本例中为 8。
这就是这个过程将如何展开。
但在这种情况下，最小化玩家考虑
他们的两个选项，然后是所有选项
结果就会发生这种情况。
现在，这就是 Minimax 算法的概况。
现在让我们尝试使用一些伪代码来形式化它。
那么 Minimax 算法到底发生了什么？
好吧，给定一个状态 S，我们需要决定要发生什么。
最大玩家——如果轮到最大玩家，那么
max 将在 S 的动作中选择一个动作 A。回忆
actions 是一个带有状态的函数
并返回我可以采取的所有可能的行动。
它告诉我所有可能的动作。
最大玩家将专门挑选
一组动作中的一个动作 A 给了我
S和A的结果min值的最高值。那么这是什么意思呢？
嗯，这意味着我想要做出给我的选择
所有动作中得分最高的，A.
但那会得到什么分数呢？
为了计算这个，我需要知道我的对手，即最小玩家，
如果他们试图最小化所产生的状态的价值，就会这样做。
所以我们说，我采取这个行动后会产生什么状态，
当最小玩家尝试时会发生什么
最小化该状态的价值？
我认为这对于我所有可能的选择都是如此。
在我考虑了所有可能的选择之后，
我选择价值最高的动作 A。
同样，最小玩家也会做同样的事情，但方向相反。
他们还将考虑他们可能采取的所有行动
如果轮到他们可以采取吗？
他们将选择具有最小的动作 A
所有选项的可能值。
他们知道所有选项的最小可能值是多少，
就是考虑最大玩家会做什么，
说，将此操作应用于当前状态的结果是什么，
然后，最大玩家会尝试做什么？
最大玩家会为该特定状态计算什么值？
所以每个人都会根据尝试估计做出决定
对方会做什么。
现在我们需要把注意力转向这两个
函数，maxValue 和 minValue。
如何实际计算状态的价值
如果你想最大化它的价值，你会如何做
如果您想最小化状态的值，请计算该状态的值？
如果你能做到这一点，那么我们就有了完整的实现
这个 Minimax 算法。
那么我们来尝试一下吧。
让我们尝试实现这个 maxValue 函数
接受一个状态并返回该状态的值作为输出
如果我想最大化国家的价值。
好吧，我首先要检查的是游戏是否结束，
因为如果比赛结束的话——
换句话说，如果该状态是最终状态——
那么这很容易。
我已经有了这个实用函数来告诉我
董事会的价值是什么。
如果游戏结束了，我就检查一下，X 赢了吗？
O赢了吗？
那是领带吗？
而效用函数只知道状态的价值是多少。
更棘手的是如果比赛还没有结束
因为那时我需要对思考进行递归推理，
我的对手下一步要做什么？
然后我想计算这个状态的值，
我希望国家的价值尽可能高。
我将在名为 v 的变量中跟踪该值。
如果我希望价值尽可能高，
我需要给 v 一个初始值。
最初，我会继续将其设置为尽可能低，
因为我还不知道我可以有哪些选择。
所以最初，我将 v 设置为负无穷大，这
看起来有点奇怪，但这里的想法
是，我希望最初的值尽可能低，
因为当我考虑自己的行为时，我总是
会尝试做得比 v 更好。如果我将 v 设置为负无穷大，
我知道我总是可以做得更好。
所以现在我考虑我的行动。
这将是某种循环，
对于国家行动中的每一个行动——
回想一下，动作是一个获取我的状态的函数
并为我提供了在该状态下可以使用的所有可能的操作。
因此，对于每一个动作，我想将其与 v 进行比较并说，
好吧，v 将等于 v 和这个表达式的最大值。
那么这个表情是什么？
嗯，首先，得到采取行动的结果和状态，
然后得到它的最小值。
换句话说，我想知道
从这个状态来看，最小玩家能做的最好的事情是什么，
因为他们会尝试将分数最小化。
因此，无论结果分数是该状态的最小值，
将其与我当前的最佳值进行比较，然后选择这两个值中的最大值，
因为我正在努力实现价值最大化。
简而言之，这三行代码是做什么的
正在检查我所有可能的行动并提出问题，
考虑到对手将要尝试做什么，我该如何最大化分数？
在整个循环之后，我可以返回 v，
这就是现在该特定状态的价值。
而对于普通玩家来说，则恰恰相反，同样的逻辑，
只是倒退。
要计算状态的最小值，
首先我们检查它是否是终止状态。
如果是，我们返回它的效用。
否则，我们现在将尝试最小化状态的价值，
考虑到我所有可能的行动。
所以我需要 v 的初始值，即状态值。
最初，我会将其设置为无穷大，因为我知道它总是可以
得到小于无穷大的东西。
因此，从 v 开始等于无穷大，我确保第一个动作
我发现 -
这将小于 v 的值。
然后我也做同样的事情——
循环遍历我所有可能的操作，并且对于每个
当最大玩家做出决定时我们可以获得的结果，
让我们取其中的最小值和 v 的当前值。
所以说完之后我得到了 v 的最小可能值，
然后我返回给用户。
因此，这实际上是 Minimax 的伪代码。
这就是我们如何进行游戏并找出最好的行动
是通过递归地使用这些 maxValue 和 minValue 函数，其中
maxValue调用minValue，minValue调用maxValue，返回
如此往复，直到我们到达最终状态，此时
我们的算法可以简单地返回该特定状态的效用。
你可能会想象这将会发生
开始是一个漫长的过程，尤其是当比赛开始时
变得更加复杂，因为我们开始添加更多动作和更多可能的选项
以及可能会持续更长时间的游戏。
那么下一个问题是，我们可以在这里进行什么样的优化？
我们如何做得更好才能使用更少的空间
或者花更少的时间就能解决此类问题？
我们将研究一些可能的优化。
但首先，我们来看看这个例子。
我们再次转向这些向上箭头和向下箭头。
让我们想象一下，我现在是最大玩家，这个绿色箭头。
我正在努力让分数尽可能高。
这是一个简单的游戏，只有两步。
我采取行动，这三个选择之一，
然后我的对手采取行动，这三个选项之一，
基于我采取的行动。
结果，我们得到了一些价值。
让我们看看我进行这些计算的顺序
并找出我是否可以进行任何优化
到这个计算过程。
我将不得不一次一个地查看这些状态。
假设我从左边开始说，好吧，现在
我要考虑的是，我的对手、最小玩家会在这里尝试做什么？
好吧，最小玩家将考虑他们所有三个可能的行动
并查看它们的值，因为这些是最终状态。
他们是游戏的终结者。
所以他们会看到，好吧，这个节点的值为 4，值为 8，
值为 5。
最小玩家会说，好吧。
在这三个选项 4、8 和 5 之间，
我选最小的，我选4的。
所以这个状态现在的值为 4。
然后我作为最大玩家说，好吧，如果我采取这个行动，
其值为 4。
这是我能做的最好的事情，因为最小的玩家
我会尝试最小化我的分数。
那么现在，如果我选择这个选项呢？
接下来我们将探讨这一点。
现在我探索如果我选择这个动作，最小玩家会做什么。
最小玩家会说，好吧，这三个选项是什么？
最小玩家有 9、3 和 7 之间的选项，所以 3
是 9、3、7 中最小的。
所以我们继续说这个状态的值为 3。
所以现在我，作为最大的球员——
我现在已经探索了三个选项中的两个。
我知道我的选择之一至少能保证我得到 4 分，
我的选择之一保证我能得到 3 分。
现在我考虑我的第三个选择并说，好吧，这里会发生什么？
同样的逻辑——最小的玩家会去
查看这三个状态 2、4 和 6，
假设最小可能的选项是 2，那么最小玩家想要两个。
现在我作为max玩家已经计算出了所有的信息
通过查看两层深度，通过查看所有这些节点。
现在我可以说，在 4、3 和 2 之间，你知道吗？
我宁愿选择4，因为如果我选择
这个选项，如果我的对手发挥最佳，
他们会尽力让我进入四号位，但这是我能做的最好的事情了。
我不能保证更高的分数，因为如果我
选择这两个选项中的一个，我可能会得到 3，也可能会得到 2。
确实，这里有一个 9，那就是
任何分数中的最高分。
所以我可能想说，你知道吗？
也许我应该选择这个选项，因为我可能会得到 9。
但如果最小玩家玩得很聪明，
他们是否在每个可能的选择中都采取了最佳行动
当他们做出选择时，我会留下3，
而我可以更好地发挥最佳水平，
我保证我会得到4。
所以这不会影响我的逻辑
用作 Minimax 玩家，试图从该节点最大化我的分数。
但事实证明，这需要大量的计算
让我弄清楚这一点。
我必须对所有这些节点进行推理才能得出这个结论。
这是一个非常简单的游戏，我有三个选择，
我的对手有三个选择，然后游戏就结束了。
所以我想做的就是想出一些方法来优化它。
也许我不需要做所有这些计算来仍然达到
结论是，你知道吗？
这个向左的动作——
这是我能做的最好的事情了。
让我们继续尝试，尝试变得更聪明一点
关于我如何去做这件事。
首先，我以完全相同的方式开始。
我一开始不知道该怎么办，所以我只是
必须考虑其中一个选项并考虑最小玩家可能会做什么。
Min 有 3 个选项：4、8 和 5。
敏说，在这三个选项中，4 是他们能做的最好的，
因为他们想尽量减少分数。
现在，我，最大的玩家，会考虑我的第二个选择，
在这里做出这一举动并考虑我的对手会如何反应。
最小玩家会做什么？
好吧，最小玩家将从该状态开始考虑他们的选择。
我想说，好吧。
9是一个选项，3是一个选项。
如果我从这个初始状态开始计算，
进行所有这些计算，当我看到 3 时，
这对我来说应该立即成为一个危险信号，
因为当我在这个状态下看到 3 时，
我知道这个状态的值最多是 3。
将会是 3 或小于 3，
即使我还没有看过最后一个动作甚至进一步的动作
如果这里可以采取更多行动。
我怎么知道呢？
好吧，我知道最小玩家会尝试最小化我的分数。
如果他们看到 3，唯一的可能是 3 以外的东西
就是如果我还没有看过的剩下的东西少于3，
这意味着这个值不可能超过 3，
因为min玩家已经可以保证3了，
他们试图最小化我的分数。
那么这告诉我什么呢？
嗯，它告诉我，如果我选择这个动作，
如果我运气不好的话，我的分数将是3，甚至可能低于3。
但我已经知道，这个行动将保证我获得 4 分。
鉴于我知道这个动作保证我得到 4 分，
这个动作意味着我不能做得比 3 更好，
如果我想最大化我的选择，那就有
我在这里不需要考虑这个三角形。
没有任何价值，没有数字可以放在这里，
这会改变我对这两种选择的看法。
我总是会选择这条让我获得4分的道路，
与这条路相反，我能做的最好的是 3，
如果我的对手发挥最佳。
对于我所看到的所有未来状态来说也是如此。
但如果我看看这里，最小玩家可能会在这里做什么，
如果我看到这个状态是 2，我就知道这个状态最多是 2，
因为这个值可能不是 2 的唯一方法
如果这些剩余状态之一小于 2，
所以最小玩家会选择它。
因此，即使不考虑这些剩余的状态，
我，作为最大化玩家，可以知道选择这条向左的路径
比选择右边两条路中的任何一条要好，
因为这个不可能比 3 更好，这个也不可能比 2 更好，
所以在这种情况下 4 是我能做的最好的。
现在我可以说这个状态的值为 4。
所以为了进行这种类型的计算，
我做了更多的簿记工作，记录事情，
一直跟踪，我能做的最好的事情是什么，
我能做的最坏的事情是什么？对于这些州，我说：
好吧，如果我已经知道我可以得到 4，
那么如果我在这个状态下能做到的最好成绩是3，
我没有理由考虑它。
我可以有效地从树上修剪这片叶子和它下面的任何东西。
正是出于这个原因，这种方法，这种对 Minimax 的优化，
称为 alpha-beta 剪枝。
Alpha和beta代表这两个值
你必须跟踪的，到目前为止你能做的最好的事情
以及迄今为止你能做的最坏的事情。
修剪的想法是，如果我有一个又大又长又深的搜索树，
如果我不这样做，我也许可以更有效地搜索它
需要搜索所有内容，如果我可以删除一些节点
尝试优化我浏览整个搜索空间的方式。
所以 alpha-beta 剪枝绝对可以节省我们很多时间
当我们通过提高搜索效率来进行搜索过程时。
但即便如此，随着游戏变得越来越复杂，它仍然不是很好。
幸运的是，井字游戏是一个相对简单的游戏，
我们可以合理地问这样的问题，
总共有多少种可能的井字游戏？
你可以考虑一下。
您可以尝试估计，在任何给定点有多少步？
游戏可以持续多少步？
事实证明，大约有 255,000 种可能的井字游戏
可以玩。
但与更复杂的游戏相比，
例如，就像国际象棋游戏一样——
更多的棋子、更多的动作、持续时间更长的游戏。
共有多少种可能的国际象棋游戏？
事实证明，每次只进行四步之后，
白人玩家走四步，黑人玩家走四步，
象棋有2880亿种可能
这种情况下可能会产生的游戏，每场只需要四步棋。
甚至更进一步。
如果你看一下整个国际象棋游戏以及有多少种可能的国际象棋游戏
可能结果是那里有超过10个
29,000 种可能的国际象棋游戏，还有更多的国际象棋游戏
比以往任何时候都可以考虑的。
这对于 Minimax 算法来说是一个相当大的问题，因为 Minimax
算法从初始状态开始，考虑所有可能的动作
以及此后所有可能的行动
直到我们到达游戏结束。
如果计算机是这样的话，这将是一个问题
需要检查这么多州，其中
远远超过任何计算机在合理时间内所能完成的任务。
那么我们该怎么做才能解决这个问题呢？
与其查看所有这些状态，
对于计算机来说这是完全棘手的，我们需要一些更好的方法。
事实证明，更好的方法通常采取某种形式
称为深度限制的极小极大。
通常 Minimax 是深度无限的——
我们继续前进，一层又一层，一步又一步，
直到游戏结束——
深度有限的 Minimax 会说，你知道吗？
经过一定次数的移动之后——也许我会
向前看 10 步，也许我会向前看 12 步，但在那之后，
我要停下来，不考虑其他的举动
在那之后可能会出现，只是因为它会
考虑所有这些可能的选项在计算上是困难的。
但是当我们深入 10 或 12 步之后我们该怎么办？
我们会遇到游戏尚未结束的情况吗？
Minimax 仍然需要一种方法来为游戏板或游戏状态分配分数
找出它的当前值是多少
如果游戏结束的话很容易做到，但事实并非如此
如果游戏还没有结束，这很容易做到。
因此，为了做到这一点，我们需要添加一项附加功能
深度限制的 Minimax 称为评估函数，
这只是一些正在进行的功能
估计给定状态下游戏的预期效用。
所以在象棋这样的游戏中，如果你想象游戏值为 1
负1表示白方胜，负1表示黑方胜，0表示平局，
那么你可能会认为 0.8 的分数意味着白色很可能获胜，
虽然肯定不能保证。
你会有一个评估函数来估计
游戏状态恰好有多好。
取决于评估函数的好坏，
这最终会限制人工智能的性能。
人工智能评估效果越好
或者任何特定的游戏状态有多糟糕，AI 就越好
将能够玩那个游戏。
如果评价函数更差并且不那么好
估计预期效用是什么，
那么事情就会变得更加困难。
您可以想象尝试提出这些评估函数。
例如，在国际象棋中，您可能会编写一个评估函数
根据您拥有的件数进行比较
到你的对手有多少棋子，因为每一棋子
在您的评估函数中具有价值。
可能还需要多一点
比考虑其他可能的情况更复杂
也可能会出现。
Minimax 还有许多其他变体
添加额外的功能以帮助其更好地执行
在这些更大且计算上更难处理的
我们不可能探索所有可能的动作的情况，
所以我们需要弄清楚如何使用评估
最终，能够玩这些游戏的功能和其他技术，
更好的。
但现在我们来看看这种对抗性搜索，这些搜索
我们遇到的问题是我正在尝试的情况
与某种对手比赛。
这些搜索问题随处可见
整个人工智能。
我们今天讨论了很多更经典的搜索问题，
就像试图找到从一个位置到另一个位置的路线一样。
但每当人工智能面临尝试做出这样的决定时，
我现在该怎么做才能做一些理性的事情，
或者做一些聪明的事情，或者尝试玩游戏，
比如弄清楚要采取什么行动，这些算法
真的可以派上用场。
事实证明，对于井字棋来说，解决方案非常简单，
因为这是一个小游戏。
XKCD 制作了网络漫画而闻名
他会准确地告诉你应该采取什么行动作为最佳行动
无论对手做什么。
这种事情不太可能
对于跳棋或国际象棋等更大的游戏，
例如，国际象棋完全是计算性的
大多数计算机都难以探索
所有可能的状态。
因此，我们确实需要人工智能更加智能地了解如何
他们开始尝试解决这些问题
以及他们如何应对这种环境
他们最终发现自己处于其中
寻找这些解决方案之一。
那么，这是对搜索和人工智能的审视。
下次我们来看看知识，
思考我们的人工智能如何能够了解信息、推理
了解这些信息并得出结论，一切都在我们对人工智能的审视中
及其背后的原则。
我们下次再见。