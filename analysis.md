# Analysis

## Layer TODO, Head TODO

Layer 11 shows [SEP] has similar possibilities for all heads except more attention to .
Layer 12 shows basically [SEP] and . pays similar attention to other tokens except for CLS.
               while for head 7, it shows more noise as . pay similar attention to all other words.
Layer 4 shows the following significantly: the head 1 and 7 is not related to other words that much while each other words pay attention more to previous word.
Generally, head 1, 4, 5 shows in this sample, it let the post word pays more attention to previous word.

Example Sentences:
- Holmes got the [MASK].
- Trump suddenly got his [MASK].

## Layer TODO, Head TODO

Example Sentences:
- Holmes successfully  got [MASK] gun.
- Trump suddenly got [MASK] Rigoletto.
Layer 4 shows the following significantly: the head 1 and 8 is not related to other words that much while each other words pay attention more to previous word.
Generally, head 3, 4, 6 shows in this sample, it let the post word pays more attention to previous word.

